{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/LogoWekeo_Copernicus_RGB_0.png' alt='' align='centre' width='30%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Copernicus data to investigate Marine Heatwaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Version: 2.0\n",
    "    Date:    29/06/2020\n",
    "    Author:  Hayley Evers-King (EUMETSAT) and Ben Loveday (Innoflair, Plymouth Marine Laboratory)\n",
    "    Credit:  This code was developed for EUMETSAT under contracts for the Copernicus \n",
    "             programme.\n",
    "    License: This code is offered as open source and free-to-use in the public domain, \n",
    "             with no warranty, under the MIT license associated with this code repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this notebook for?**\n",
    "\n",
    "This notebook will download EUMETSAT Sentinel-3 SLSTR data for composite plotting, as well as CMEMS time series data of SST, from both reprocessed and NRT data stream. The notebook covers the application of some simple plotting techniques and application of basic analysis to investigate both historical and current potential for the occurrence of marine heat waves.\n",
    "\n",
    "**What specific tools does this notebook use?**\n",
    "\n",
    "Beyond general Python modules, this notebook imports some functions for managing the harmonised data access api (harmonised_data_access_api_tools.py) which can be found in the wekeo-hda folder on JupyterLab, and additional libraries for marine heatwave analysis provided openly and based on the work of <a href=\"https://github.com/ecjoliver/marineHeatWaves\"> Hobday et al.</a>.\n",
    "\n",
    "**What are marine heatwaves and how can Copernicus data be used to observe them?**\n",
    "\n",
    "Like heatwaves on land, marine heatwaves are extended periods of higher than normal temperatures. They have occurred in different areas around the world and can be caused by different oceanographic driving forces. You can find out all about them <a href=\"http://www.marineheatwaves.org/all-about-mhws.html\">here</a>. \n",
    "\n",
    "\n",
    "The variable drivers, and historical context for defining heatwaves regionally, mean that we need the ability to measure the range of ocean temperatures that occur all over the world at any given time, and also a long-term base-line understanding of what ‘normal’ temperatures look like.\n",
    "\n",
    "Sea surface temperature measurements from satellites can support the identification of marine heatwaves, both through the daily measurements they make, and their contributions to long-term data records. The Sentinel-3 satellites are the Copernicus programme's contribution to climate-scale monitoring of sea surface temperatures, with the Sea and Land Surface Temperature Radiometer (SLSTR) on Sentinel-3A now able to function as a reference sensor, when combining multiple sea surface temperature data sets, such as those available from the <a href=\"http://marine.copernicus.eu\">Copernicus Marine Service</a>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started! Python is divided into a series of modules that each contain a series of methods for specific tasks. The box below imports all of the moduls we need to complete our plotting task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# standard tools\n",
    "import os, sys, json\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "import logging\n",
    "from IPython.core.display import HTML\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patheffects as path_effects\n",
    "import xmltodict\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# HDA API tools\n",
    "sys.path.append(os.path.abspath('..\\\\..\\\\wekeo-hda'))\n",
    "#sys.path.append(os.path.abspath('../../../wekeo-hda'))\n",
    "from importnb import Notebook\n",
    "with Notebook(): \n",
    "    import hda_api_functions as hapi\n",
    "        \n",
    "# specific tools (which can be found here ../../tools/)\n",
    "sys.path.append(os.path.join(os.getcwd(),'tools'))\n",
    "import image_tools as img\n",
    "sys.path.append(os.path.join(os.getcwd(),'tools','mhw_master'))\n",
    "import marineHeatWaves as mhw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEkEO provides access to a huge number of datasets through its **'harmonised-data-access'** API. This allows us to query the full data catalogue and download data quickly and directly onto the Jupyter Lab. You can search for what data is available <a href=\"https://wekeo.eu/data?view=catalogue\">here</a>\n",
    "\n",
    "In order to use the HDA-API we need to provide some authentication credentials, which comes in the form of an API key and API token. In this notebook we have provided functions so you can retrieve the API key and token you need directly. You can find out more about this process in the notebook on HDA access (wekeo_harmonized_data_access_api.ipynb) that can be found in the **wekeo-hda** folder on your Jupyterlab.\n",
    "\n",
    "We will also define a few other parameters including where to download the data to, and if we want the HDA-API functions to be verbose. **Lastly, we will tell the notebook where to find the query we will use to find the data.** These 'JSON' queries are what we use to ask WEkEO for data. They have a very specific form, but allow us quite fine grained control over what data to get. You can find the ones that we will use here: **JSON_templates/mhw/..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your WEkEO API username and password (needs to be in '  ')\n",
    "user_name = '<YOUR USER NAME>'\n",
    "password = '<YOUR PASSWORD>'\n",
    "\n",
    "# Generate an API key\n",
    "api_key = hapi.generate_api_key(user_name, password)\n",
    "display(HTML('Your API key is: <b>'+api_key+'</b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the data should be downloaded to:\n",
    "download_dir_path = os.getcwd()\n",
    "# where we can find our data query form:\n",
    "JSON_query_dir = os.path.join(os.getcwd(),'JSON_templates','mhw')\n",
    "# HDA-API loud and noisy?\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to get our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLSTR L2 SST KEY\n",
    "dataset_id = \"EO:EUM:DAT:SENTINEL-3:SL_2_WST___\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our dataset ID to load the correct JSON query file from ../JSON_templates/mhw/\n",
    "\n",
    "You can edit this query if you want to get different data, but be aware of asking for too much data - you could be here a while and might run out of space to use this data in the JupyterLab. The box below gets the correct query file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find query file\n",
    "JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "if not os.path.exists(JSON_query_file):\n",
    "    print('Query file ' + JSON_query_file + ' does not exist')\n",
    "else:\n",
    "    print('Found JSON query file for '+dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a query, we need to launch it to WEkEO to get our data. The box below takes care of this through the following steps:\n",
    "    1. initialise our HDA-API\n",
    "    2. get an access token for our data\n",
    "    3. accepts the WEkEO terms and conditions\n",
    "    4. loads our JSON query into memory\n",
    "    5. launches our search\n",
    "    6. waits for our search results\n",
    "    7. gets our result list\n",
    "    8. downloads our data\n",
    "\n",
    "This is quite a complex process, so much of the functionality has been buried 'behind the scenes'. If you want more information, you can check out the <a href=\"./wekeo-hda\">Harmonised Data Access API </a></span> tutorials. The code below will report some information as it runs. At the end, it should tell you that one product has been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path)\n",
    "HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "HAPI_dict = hapi.acceptTandC(HAPI_dict)\n",
    "\n",
    "# load the query\n",
    "with open(JSON_query_file, 'r') as f:\n",
    "    query = json.load(f)\n",
    "\n",
    "# launch job\n",
    "print('Launching job...')\n",
    "HAPI_dict = hapi.get_job_id(HAPI_dict, query)\n",
    "\n",
    "# check results\n",
    "print('Getting results...')\n",
    "HAPI_dict = hapi.get_results_list(HAPI_dict)\n",
    "HAPI_dict = hapi.get_order_ids(HAPI_dict)\n",
    "\n",
    "# download data\n",
    "print('Downloading data...')\n",
    "hapi.download_data(HAPI_dict, skip_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel data is usually distributed as a zip file, which contains the SAFE format data within. To use this, we must unzip the file. The bow below handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip file\n",
    "for filename in HAPI_dict['filenames']:\n",
    "    if os.path.splitext(filename)[-1] == '.zip':\n",
    "        print('Unzipping file')\n",
    "        with ZipFile(filename, 'r') as zipObj:\n",
    "            # Extract all the contents of zip file in current directory\n",
    "            zipObj.extractall(os.path.dirname(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting SLSTR data\n",
    "\n",
    "We plot our SLSTR scenes using a function that manages data ingestion, flagging, bias correction and makes some map embelishements (e.g. adds dotted lines to the scene edges, so we can tell where the boundaries are). We call this function for each image in the boxes further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_SLSTR_composite_plot(nc_files, plot_extents=None, fsz=20,\\\n",
    "                              land_resolution='50m', vmin=10, vmax=28,\\\n",
    "                              xsize=20, ysize=16, dpi=150, QMASK=4,\\\n",
    "                              cmap=plt.cm.RdYlBu_r):\n",
    "    '''\n",
    "     Plots SLSTR images from n input files, will overay first to last.\n",
    "    '''\n",
    "    # get cartopy land layers\n",
    "    land_poly = cfeature.NaturalEarthFeature('physical', 'land', land_resolution,\n",
    "                                             edgecolor='k',\n",
    "                                             facecolor=cfeature.COLORS['land'])\n",
    "    \n",
    "    # setup figure\n",
    "    fig = plt.figure(figsize=(xsize, ysize), dpi=dpi)\n",
    "    plt.rc('font', size=fsz)\n",
    "    \n",
    "    # setup axes\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[20,0.5,1])\n",
    "    gs.update(wspace=0.01, hspace=0.01)\n",
    "\n",
    "    # setup plot 1\n",
    "    axes_m = plt.subplot(gs[0,0], projection=ccrs.PlateCarree())\n",
    "    axes_m.background_patch.set_facecolor('0.5')\n",
    "\n",
    "\n",
    "    if plot_extents:\n",
    "        axes_m.set_extent(plot_extents, ccrs.PlateCarree())\n",
    "\n",
    "    # read data and plot background\n",
    "    for nc_file in nc_files:\n",
    "        nc_fid = xr.open_dataset(nc_file)\n",
    "        LON = nc_fid.lon.data\n",
    "        LAT = nc_fid.lat.data\n",
    "        \n",
    "        x1 = 0 ; x2 = -1; y1 = 0 ; y2 = -1\n",
    "        # get subset based on plot extents\n",
    "        if plot_extents:\n",
    "            x1, x2, y1, y2 = img.subset_image(LAT, LON, plot_extents)\n",
    "\n",
    "        # read in and subset if required\n",
    "        LON = LON[x1:x2,y1:y2]\n",
    "        LAT = LAT[x1:x2,y1:y2]\n",
    "        SST_raw = np.squeeze(nc_fid.sea_surface_temperature.data[0,x1:x2,y1:y2])\n",
    "        SST_BIAS = np.squeeze(nc_fid.sses_bias.data[0,x1:x2,y1:y2])\n",
    "        QUALITY_LEVEL = np.squeeze(nc_fid.quality_level.data[0,x1:x2,y1:y2])\n",
    "        nc_fid.close()\n",
    "        \n",
    "        # correct SST\n",
    "        SST_C = SST_raw + SST_BIAS - 273.15\n",
    "\n",
    "        # flag data\n",
    "        SST_C[QUALITY_LEVEL<=QMASK] = np.nan\n",
    "\n",
    "        # plot the SST field\n",
    "        p1 = axes_m.pcolormesh(LON, LAT, SST_C, cmap=cmap,\\\n",
    "                          vmin=vmin, vmax=vmax, zorder=-1)\n",
    "\n",
    "        # add the plot edges\n",
    "        xml_file = os.path.dirname(nc_file)+'/xfdumanifest.xml'\n",
    "        with open(xml_file) as fd:\n",
    "            doc = xmltodict.parse(fd.read())\n",
    "            coords = doc['xfdu:XFDU']['metadataSection']['metadataObject'][2]\\\n",
    "                        ['metadataWrap']['xmlData']['sentinel-safe:frameSet']\\\n",
    "                        ['sentinel-safe:footPrint']['gml:posList']\n",
    "\n",
    "        # split the coords\n",
    "        lats_lons = coords.split(' ')\n",
    "        lats = np.asarray(lats_lons[::2]).astype(float)\n",
    "        lons = np.asarray(lats_lons[1::2]).astype(float)\n",
    "\n",
    "        if 'S3B' in nc_file:\n",
    "            plot_col = 'b'\n",
    "        else:\n",
    "            plot_col = 'k'\n",
    "\n",
    "        p2 = axes_m.plot(lons, lats, c=plot_col, linewidth=1,\\\n",
    "                    linestyle='--', zorder=5, alpha=0.5,\\\n",
    "                    transform=ccrs.PlateCarree())\n",
    "\n",
    "    # add some map embelishments\n",
    "    axes_m.coastlines(resolution=land_resolution, color='black', linewidth=1)\n",
    "    axes_m.add_feature(land_poly)\n",
    "    g1 = axes_m.gridlines(draw_labels = True)\n",
    "    g1.xlabels_top = False\n",
    "    g1.ylabels_right = False\n",
    "\n",
    "    g1.xlabel_style = {'size': fsz, 'color': 'gray'}\n",
    "    g1.ylabel_style = {'size': fsz, 'color': 'gray'}\n",
    "\n",
    "    # setup plot2: colorbar\n",
    "    axes_c = plt.subplot(gs[2,0])\n",
    "    cbar = plt.colorbar(p1, cax=axes_c, orientation='horizontal')\n",
    "    cbar.ax.tick_params(labelsize=fsz) \n",
    "    cbar.set_label('SLSTR SST composite [$^{o}$C]',fontsize=fsz)\n",
    "    \n",
    "    return gs, axes_m, axes_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by finding all the necessary files (which glob.glob takes care of). Th files are added to a list which is then sent to our large function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbosity\n",
    "verbose = False\n",
    "\n",
    "# figure options\n",
    "figure_font_size = 20\n",
    "plot_extents = [-160, -116, 10, 45]\n",
    "vmin = 10\n",
    "vmax = 28\n",
    "\n",
    "# get the files\n",
    "SLSTR_files = glob.glob(os.path.join(download_dir_path,'S3*WST*201909*','*.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we pass our list of files to the plotting routine. The plotting routine returns the handles of our axes, so that we can still make some changes once the main plot is done (e.g. add the annotations). Finally, it will save fie figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the plot: we will call this as a function as in contains a 'for' loop to make the plot\n",
    "fig, axis, colbar = make_SLSTR_composite_plot(SLSTR_files, plot_extents=plot_extents,\\\n",
    "                                              fsz=figure_font_size, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# add some embellishments\n",
    "plt.sca(axis)\n",
    "\n",
    "label='Sentinel-3A SLSTR SST\\n07/09/2019 (22:27 local time)'    \n",
    "txt = plt.annotate(label, xy=(0.66, 1.01), xycoords='axes fraction',\\\n",
    "               size=figure_font_size/1.25,\n",
    "               color='k', zorder=100, annotation_clip=False)\n",
    "\n",
    "label='Sentinel-3A SLSTR SST\\n08/09/2019 (00:08 local time)'    \n",
    "txt = plt.annotate(label, xy=(0.005, 1.01), xycoords='axes fraction',\\\n",
    "               size=figure_font_size/1.25,\n",
    "               color='k', zorder=100, annotation_clip=False)\n",
    "\n",
    "label='Sentinel-3B SLSTR SST\\n08/09/2019 (23:28 local time)'    \n",
    "txt = plt.annotate(label, xy=(0.33, 1.01), xycoords='axes fraction',\\\n",
    "               size=figure_font_size/1.25,\n",
    "               color='b', zorder=100, annotation_clip=False)\n",
    "\n",
    "plt.savefig('SLSTR_All_SST_California_20190909.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now find the image in the folder where your code is stored in your instance of the JupyterLab. The image is a composite of three images from the SLSTR sensors aboard the Sentinel-3A and B satellites which captured warm temperatures around the eastern Pacific in September 2019. This highlights the increased coverage that can be achieved in both time and space, using multiple sensors, but in order to understand if these temperatures could be an indicator of the beginning of a marine heatwave we need some longer term context..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at time series data of sea surface temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To place the images above in context we'll need to look at a time series of data. For this we can access data from the Copernicus services, where multiple data sources (different satellites etc) are combined to produce data sets that cover longer time periods. In this case we are going to look at the OSTIA reprocessed SST, as well as the near real time (NRT)processing of the same product, so we can look at data right up to the time period of the SLSTR images we looked at previously. \n",
    "\n",
    "Reprocessed and NRT data streams are produced separately and we must be careful when we interpret these data sets together. As time progresses, we understand better how satellite instruments perform, algorithms improve, and data is reprocessed to ensure good quality and consistency between sources. With NRT we do not have all the information we have in hindsight, particularly about instrument characterisation, but this data is vitally important for understanding events that are happening right now. So, for example, you would not use combined NRT and reprocessed data sets for long-term trend analysis, and you would want to consider NRT measurements with a higher degree of uncertainty that you might consider with reprocessed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will construct and submit a query to the WEkEO Harmonised Data Access API to get this data. Here we have supplied the JSON files for the data sets of interest, but you could edit these files to look at your own time frames/regions of interest etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find query file for OSTIA NRT and initialise dictionary\n",
    "dataset_id = \"EO:MO:DAT:SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001\"\n",
    "\n",
    "# makes a date array...NfH\n",
    "start_dates = []\n",
    "end_dates = []\n",
    "for ii in range(2008,2019+1):\n",
    "    start_dates.append(str(ii)+\"-01-01T00:00:00.000Z\")\n",
    "    end_dates.append(str(ii)+\"-12-31T00:00:00.000Z\")\n",
    "\n",
    "# find query file\n",
    "JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "if not os.path.exists(JSON_query_file):\n",
    "    print('Query file ' + JSON_query_file + ' does not exist')\n",
    "else:\n",
    "    print('Found JSON query file for '+dataset_id)\n",
    "\n",
    "# init HAPI\n",
    "HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path)\n",
    "HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "HAPI_dict = hapi.acceptTandC(HAPI_dict)\n",
    "\n",
    "# start loop over dates\n",
    "for start_date, end_date in zip(start_dates, end_dates):\n",
    "    print('Running for: ' + start_date + ' to ' + end_date)\n",
    "\n",
    "    date_string = start_date.split('T')[0].replace('-','') \\\n",
    "                  + '_' + end_date.split('T')[0].replace('-','')\n",
    "    \n",
    "    # load the query\n",
    "    with open(JSON_query_file, 'r') as f:\n",
    "        query = f.read()\n",
    "        query = query.replace(\"%DATE_START%\",start_date)\n",
    "        query = query.replace(\"%DATE_END%\",end_date)\n",
    "        query = json.loads(query)\n",
    "\n",
    "    # launch job\n",
    "    print('Launching job...')\n",
    "    HAPI_dict = hapi.get_job_id(HAPI_dict, query)\n",
    "\n",
    "    # check results\n",
    "    print('Getting results...')\n",
    "    HAPI_dict = hapi.get_results_list(HAPI_dict)\n",
    "    HAPI_dict = hapi.get_order_ids(HAPI_dict)\n",
    "\n",
    "    # download data\n",
    "    print('Downloading data...')\n",
    "    hapi.download_data(HAPI_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find query file for OSTIA reprocessed and initialise dictionary\n",
    "\n",
    "dataset_id = \"EO_MO_DAT_SST_GLO_SST_L4_REP_OBSERVATIONS_010_011\"\n",
    "\n",
    "# makes a date array...NfH\n",
    "start_dates = []\n",
    "end_dates = []\n",
    "for ii in range(1988,2007+1):\n",
    "    start_dates.append(str(ii)+\"-01-01T00:00:00.000Z\")\n",
    "    end_dates.append(str(ii)+\"-12-31T00:00:00.000Z\")\n",
    "\n",
    "# find query file\n",
    "JSON_query_file = os.path.join(JSON_query_dir,dataset_id.replace(':','_')+\".json\")\n",
    "if not os.path.exists(JSON_query_file):\n",
    "    print('Query file ' + JSON_query_file + ' does not exist')\n",
    "else:\n",
    "    print('Found JSON query file for '+dataset_id)\n",
    "\n",
    "# init HAPI\n",
    "HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path)\n",
    "HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "HAPI_dict = hapi.acceptTandC(HAPI_dict)\n",
    "\n",
    "# start loop over dates\n",
    "for start_date, end_date in zip(start_dates, end_dates):\n",
    "    print('Running for: ' + start_date + ' to ' + end_date)\n",
    "\n",
    "    date_string = start_date.split('T')[0].replace('-','') \\\n",
    "                  + '_' + end_date.split('T')[0].replace('-','')\n",
    "    \n",
    "    # load the query\n",
    "    with open(JSON_query_file, 'r') as f:\n",
    "        query = f.read()\n",
    "        query = query.replace(\"%DATE_START%\",start_date)\n",
    "        query = query.replace(\"%DATE_END%\",end_date)\n",
    "        query = json.loads(query)\n",
    "\n",
    "    # launch job\n",
    "    print('Launching job...')\n",
    "    HAPI_dict = hapi.get_job_id(HAPI_dict, query)\n",
    "\n",
    "    # check results\n",
    "    print('Getting results...')\n",
    "    HAPI_dict = hapi.get_results_list(HAPI_dict)\n",
    "    HAPI_dict = hapi.get_order_ids(HAPI_dict)\n",
    "\n",
    "    # download data\n",
    "    print('Downloading data...')\n",
    "    hapi.download_data(HAPI_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting time series data to investigate occurrence and potential for marine heatwaves "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will set up some parameters including times and space we want to work over when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# subset image: cut a relevant section out of an image for area averaging. If false, whole area will be used.\n",
    "# THERE IS NO AREA WEIGHTING IN THE AVERAGING HERE! \n",
    "# Subset_extents [lon1,lon2,lat1,lat2] describes the section we cut\n",
    "subset_image = False\n",
    "subset_extents = [-165.0, -145.0, 10.0, 30.0]\n",
    "\n",
    "# The data product time is measured in seconds since the following refernce date\n",
    "Date_ref = datetime.datetime(1981,1,1)\n",
    "\n",
    "# Select the times we want to use for our spatial anomaly plots [month, day]\n",
    "month_day_start = [8,1] \n",
    "month_day_end = [9,24]\n",
    "\n",
    "# Plotting font size\n",
    "fsz = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will find our datasets and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_files = []\n",
    "my_files = glob.glob(download_dir_path+'/METOFFICE-GLO-SST-L4-RAN*')\n",
    "for SST_file in sorted(my_files):\n",
    "    SST_files.append(SST_file)\n",
    "    \n",
    "my_files = glob.glob(download_dir_path+'/METOFFICE-GLO-SST-L4-NRT*')\n",
    "for SST_file in sorted(my_files):\n",
    "    SST_files.append(SST_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs the bulk of the processing work for our plot. We start by loading in coordinates that we can use to subset the data (if required) and make our plots. Subsequently, we loop through the SST products and read in the data, correct Kelvin to Celsius and perform our averaging in either time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the co-ordinate variables we need for subsetting/plotting\n",
    "ds = xr.open_dataset(SST_files[-1])\n",
    "lat = ds.lat.data\n",
    "lon = ds.lon.data\n",
    "ds.close()\n",
    "\n",
    "# subset coords if required, getting indices to subset out output SST products\n",
    "if subset_image:\n",
    "    ii = np.where((lon >= subset_extents[0]) & (lon <= subset_extents[1]))[0]\n",
    "    jj = np.where((lat >= subset_extents[2]) & (lat <= subset_extents[3]))[0]\n",
    "    lon = lon[ii]\n",
    "    lat = lat[jj]\n",
    "\n",
    "# initialise lists for output times series variables for MWH    \n",
    "all_times = []\n",
    "all_SST = []\n",
    "\n",
    "# initialise arrays for output SST fields\n",
    "iter_SST = np.ones([len(SST_files), len(lat), len(lon)])*np.nan\n",
    "\n",
    "# now we get the area-averaged data\n",
    "count = -1\n",
    "for SST_file in SST_files:\n",
    "    print(SST_file)\n",
    "    count = count + 1\n",
    "\n",
    "    # xarray does not read times consistency between RAN and NRT, so we load as integer\n",
    "    ds = xr.open_dataset(SST_file, decode_times=False)\n",
    "    this_SST = ds.analysed_sst.data\n",
    "    times = ds.time.data\n",
    "    ds.close()\n",
    "\n",
    "    my_times = []\n",
    "    for time in times:\n",
    "        my_times.append(Date_ref + datetime.timedelta(seconds=int(time)))\n",
    "    times = np.asarray(my_times)\n",
    "    \n",
    "    t0 = datetime.datetime(times[0].year, month_day_start[0], month_day_start[1])\n",
    "    t1 = datetime.datetime(times[0].year, month_day_end[0], month_day_end[1])\n",
    "    tt = np.where((times >= t0) & (times <= t1))[0]\n",
    "\n",
    "    if np.nanmean(this_SST) > 100:\n",
    "        this_SST = this_SST - 273.15\n",
    "\n",
    "    if subset_image:\n",
    "        this_SST = this_SST[:,jj[0]:jj[-1],ii[0]:ii[-1]]\n",
    "    \n",
    "    time_subset_SST = this_SST[tt,:,:]\n",
    "\n",
    "    iter_SST[count,:,:] = np.squeeze(np.nanmean(time_subset_SST, axis=0))\n",
    "\n",
    "    all_times.append(my_times)\n",
    "    all_SST.append(np.nanmean(np.nanmean(this_SST, axis=1), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit of formatting is necessary for the plots we want to make..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the SST list\n",
    "SST_time_series = [item for sublist in all_SST for item in sublist]\n",
    "SST_time_series = np.asarray(SST_time_series)\n",
    "\n",
    "# format the dates for the MWH toolkit\n",
    "Dates_time_series = [item for sublist in all_times for item in sublist]\n",
    "Dates_time_series_formatted = [datetime.date.toordinal(tt) for tt in Dates_time_series]\n",
    "\n",
    "# make climatology of time subset region\n",
    "clim_SST = np.nanmean(iter_SST, axis=0)\n",
    "\n",
    "# calculate heat waves\n",
    "mhws, clim = mhw.detect(np.asarray(Dates_time_series_formatted), SST_time_series)\n",
    "\n",
    "# make matrix\n",
    "stripe_array = np.ones([20,len(SST_files)])*np.nan\n",
    "\n",
    "# now make the anomaly\n",
    "for ii in range(len(SST_files)):\n",
    "    stripe_array[:, ii] = np.nanmean(np.squeeze(iter_SST[ii,:,:]) - clim_SST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot we'll make, shows ‘stripes’ of the average sea surface temperature anomaly for the region around Hawaii. Recently, ‘climate stripes’  have been used by 'citizen scientists' all over the world to show long-term trends in regional temperatures. The plot below shows a stripes-style graphic for the eastern Pacific region around Hawaii. High anomalies are apparent during 2014, in 2015 during the previous marine heatwave, and were associated with reports in 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,10), dpi =300)\n",
    "vmax = np.nanmax(abs(stripe_array))\n",
    "\n",
    "date_ticks = []\n",
    "for ii in range(len(SST_files)):\n",
    "    date_ticks.append(str(1988+ii))\n",
    "    \n",
    "plt.pcolormesh(stripe_array,vmin=vmax*-1,vmax=vmax,cmap=plt.cm.RdBu_r)\n",
    "plt.xticks(np.arange(len(SST_files))+0.5,date_ticks, rotation='90')\n",
    "plt.yticks([],[])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('SST anomaly [$^{o}$C] (1$^{st}$ Aug - 24$^{th}$ Sep)',fontsize=fsz/1.25)\n",
    "plt.savefig('Stripes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will look a little more quantitatively at historical marine heatwaves in this region, and see how the situation in 2019 compared (with the previously mentioned assumptions about the accuracy of NRT data for longer term analysis). The plot below is generated using a \n",
    "a toolkit very kindly provided as open source code by Hobday et al (you can find out more information on the toolkit and the science behind it <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0079661116000057\">here</a> and <a href=\"https://github.com/ecjoliver/marineHeatWaves\">here</a>) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MWHs\n",
    "ev = np.argmax(mhws['intensity_max']) # Find largest event\n",
    "\n",
    "fig = plt.figure(figsize=(35,15), dpi = 300)\n",
    "plt.rc('font', size=fsz)\n",
    "\n",
    "# Find indices for all n MHWs before and after event of interest and shade accordingly\n",
    "n=10\n",
    "for ev0 in np.arange(ev-n, ev+n, 1):\n",
    "    t1 = np.where(Dates_time_series_formatted==mhws['time_start'][ev0])[0][0]\n",
    "    t2 = np.where(Dates_time_series_formatted==mhws['time_end'][ev0])[0][0]\n",
    "    p1 = plt.fill_between(Dates_time_series[t1:t2+1], SST_time_series[t1:t2+1],\\\n",
    "                          clim['thresh'][t1:t2+1], color=(1,0.85,0.85))\n",
    "    \n",
    "# Find indices for MHW of interest and shade accordingly\n",
    "t1 = np.where(Dates_time_series_formatted==mhws['time_start'][-1])[0][0]\n",
    "t2 = np.where(Dates_time_series_formatted==mhws['time_end'][-1])[0][0]\n",
    "p2 = plt.fill_between(Dates_time_series[t1:t2+1], SST_time_series[t1:t2+1],\\\n",
    "                      clim['thresh'][t1:t2+1], color='r')\n",
    "\n",
    "# Plot SST, seasonal cycle, threshold, shade MHWs with main event in red\n",
    "p3, = plt.plot(Dates_time_series, SST_time_series, 'k-', linewidth=2)\n",
    "p4, = plt.plot(Dates_time_series, clim['thresh'], 'b--', linewidth=2)\n",
    "p5, = plt.plot(Dates_time_series, clim['seas'], 'b-', linewidth=2)\n",
    "\n",
    "xmin = datetime.datetime(2014,1,1).toordinal() - datetime.datetime(1,1,1).toordinal()\n",
    "xmax = datetime.datetime(2019,12,31).toordinal() - datetime.datetime(1,1,1).toordinal()\n",
    "plt.xlim(xmin,xmax)\n",
    "\n",
    "plt.ylim(clim['seas'].min() - 0.3, clim['seas'].max() + mhws['intensity_max'][ev] + 0.2)\n",
    "plt.ylabel('SST [$^\\circ$C]')\n",
    "plt.annotate('Plotting script credit:\\nhttps://github.com/ecjoliver/marineHeatWaves',\\\n",
    "             (0.005, 0.935), xycoords='axes fraction', color='0.5', fontsize=fsz/1.25)\n",
    "\n",
    "leg = plt.legend([p3, p5, p4, p2, p1],\\\n",
    "                 ['OSTIA NRT SST','SST Seas. clim.','SST Seas. thresh.','Current heatwave','Past heatwave'],\\\n",
    "                 bbox_to_anchor=(1.0, -0.05), ncol=5)\n",
    "\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "plt.savefig('MHW.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/all_partners_wekeo.png' alt='' align='center' width='75%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left;\">This project is licensed under the <a href=\"./LICENSE\">MIT License</a> <span style=\"float:right;\"><a href=\"https://github.com/wekeo/wekeo-jupyter-lab\">View on GitHub</a> | <a href=\"https://www.wekeo.eu/\">WEkEO Website</a> | <a href=mailto:support@wekeo.eu>Contact</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
